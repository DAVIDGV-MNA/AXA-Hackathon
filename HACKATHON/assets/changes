def build_or_update_store():
    index, meta = load_store()
    prior_dim = _read_dim()

    # Load existing embedding matrix if present
    emb_mat = _load_embeddings()
    if emb_mat is not None and (index is None or (hasattr(index, "ntotal") and index.ntotal != emb_mat.shape[0])):
        # If counts disagree, we’ll rebuild embeddings from meta
        emb_mat = None

    indexed_ids = {m["id"] for m in meta}
    all_files = discover_files()
    new_chunks = []

    for f in all_files:
        for ch in load_file_to_chunks(f):
            if ch["id"] in indexed_ids:
                continue
            new_chunks.append(ch)

    # Nothing new—still ensure DF exists
    if not new_chunks and index is not None:
        _rebuild_df(meta, emb_mat)
        save_metadata_jsonl(meta)
        return index, meta

    # If we got here and dimension mismatch is detected, full rebuild
    def embed_chunks(chs: List[Dict[str, Any]]) -> np.ndarray:
        arr = embed_texts([c["text"] for c in chs]).astype(np.float32)
        faiss.normalize_L2(arr)
        return arr

    if new_chunks:
        new_embs = embed_chunks(new_chunks)
        dim = int(new_embs.shape[1])

        if prior_dim is not None and index is not None and dim != prior_dim:
            # Dimension changed => re-embed EVERYTHING
            index = None
            meta_all = []
            for f in all_files:
                meta_all.extend(load_file_to_chunks(f))
            all_embs = embed_chunks(meta_all)
            dim = int(all_embs.shape[1])
            index = faiss.IndexFlatIP(dim)
            index.add(all_embs)
            meta = meta_all
            _write_dim(dim)
            _save_embeddings(all_embs)
            save_store(index, meta)
            save_metadata_jsonl(meta)
            _rebuild_df(meta, all_embs)
            return index, meta

        # Normal incremental path
        if index is None:
            index = faiss.IndexFlatIP(int(new_embs.shape[1]))
            _write_dim(int(new_embs.shape[1]))

        index.add(new_embs)
        meta.extend(new_chunks)

        # Append to embedding matrix (or initialize)
        if emb_mat is None:
            emb_mat = new_embs
        else:
            emb_mat = np.vstack([emb_mat, new_embs])

        save_store(index, meta)
        _save_embeddings(emb_mat)
        save_metadata_jsonl(meta)
        _rebuild_df(meta, emb_mat)
        return index, meta

    # If we had no new chunks (should have been caught above), still build DF
    _rebuild_df(meta, emb_mat)
    save_metadata_jsonl(meta)
    return index, meta