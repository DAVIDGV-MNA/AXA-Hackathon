# ---------- create_service.py (proxy-only) ----------
# Purpose: independent service for Create-mode Assessment + Verifier
# Calls SecureGPT exclusively through the local proxy endpoints.
#
# Env vars (proxy-only, required):
#   SECUREGPT_AUTH_URL="https://onelogin.stg.axa.com/ss/token.oauth2"
#   SECUREGPT_SCOPE="urn:prch:prchtpt"
#   CLIENT_ID="..."
#   CLIENT_SECRET="..."
#   SECUREGPT_API_VERSION="2024-10-21"
#   SECUREGPT_PROXY_EMBED_URL="http://127.0.0.1:3001/securegpt/embeddings"
#   SECUREGPT_PROXY_CHAT_URL="http://127.0.0.1:3001/securegpt/chat"
#   # The proxy often needs the upstream deployment URL forwarded (model_url):
#   SECUREGPT_EMBED_MODEL_URL="https://api-int.../deployments/ada-search-index-v1/embeddings"
#   SECUREGPT_CHAT_MODEL_URL="https://api-int.../deployments/gpt-4o-.../chat/completions"
#
# Optional:
#   REQUESTS_VERIFY_TLS=false           # if your corp TLS breaks verification
#   UPLOAD_DIR="./uploads"             # not used directly here, but kept for symmetry
#   VECTOR_DIR=".vectorstore"          # expects docmeta.jsonl + embeddings.npy

from __future__ import annotations
import os, re, json, uuid, time, difflib
from pathlib import Path
from typing import Any, Dict, List, Optional

import numpy as np
import requests
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

# -------------------------
# Config (proxy-only)
# -------------------------
UPLOAD_DIR  = Path(os.getenv("UPLOAD_DIR", "./uploads"))
VECTOR_DIR  = Path(os.getenv("VECTOR_DIR", ".vectorstore"))
META_PATH   = VECTOR_DIR / "docmeta.jsonl"   # [{id,name,path,sha256,embedding}]
EMB_PATH    = VECTOR_DIR / "embeddings.npy"  # matrix [N, D] float32

SECUREGPT_AUTH_URL         = os.getenv("SECUREGPT_AUTH_URL", "")
SECUREGPT_SCOPE            = os.getenv("SECUREGPT_SCOPE", "")
SECUREGPT_CLIENT_ID        = os.getenv("CLIENT_ID", "")
SECUREGPT_CLIENT_SECRET    = os.getenv("CLIENT_SECRET", "")
SECUREGPT_API_VERSION      = os.getenv("SECUREGPT_API_VERSION", "2024-10-21")

# PROXY: required, no fallback
SECUREGPT_PROXY_EMBED_URL  = os.getenv("SECUREGPT_PROXY_EMBED_URL", "")
SECUREGPT_PROXY_CHAT_URL   = os.getenv("SECUREGPT_PROXY_CHAT_URL", "")

# Upstream deployment URLs (forwarded in payload as model_url so proxy can route)
SECUREGPT_EMBED_MODEL_URL  = os.getenv("SECUREGPT_EMBED_MODEL_URL", "")
SECUREGPT_CHAT_MODEL_URL   = os.getenv("SECUREGPT_CHAT_MODEL_URL", "")

REQUESTS_VERIFY_TLS        = os.getenv("REQUESTS_VERIFY_TLS", "false").lower() not in {"0","false","no"}

assert SECUREGPT_PROXY_EMBED_URL, "SECUREGPT_PROXY_EMBED_URL is required (proxy-only)."
assert SECUREGPT_PROXY_CHAT_URL,  "SECUREGPT_PROXY_CHAT_URL is required (proxy-only)."

# -------------------------
# App & CORS
# -------------------------
app = FastAPI(title="CreateMode Service (Proxy-Only)", version="1.1")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],   # dev-friendly; tighten later
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

_session = requests.Session()

def _common_headers() -> Dict[str, str]:
    return {
        "Content-Type": "application/json",
        "sistema": "AXA",
        "usuario": os.getenv("AXA_USER_ID", "MX000000000A"),
        "UUID": str(uuid.uuid4()),
        "fechaHora": time.strftime("%Y-%m-%dT%H:%M:%S-05:00"),
    }

def _get_auth_token() -> str:
    assert SECUREGPT_AUTH_URL and SECUREGPT_CLIENT_ID and SECUREGPT_CLIENT_SECRET, \
        "Missing AUTH_URL/CLIENT_ID/CLIENT_SECRET"
    payload = {
        "client_id": SECUREGPT_CLIENT_ID,
        "client_secret": SECUREGPT_CLIENT_SECRET,
        "scope": SECUREGPT_SCOPE,
        "grant_type": "client_credentials",
    }
    headers = {
        "Content-Type": "application/x-www-form-urlencoded",
        "usuario": os.getenv("AXA_USER_ID", "MX000000000A"),
        "sistema": "AXA",
        "UUID": str(uuid.uuid4()),
        "fechaHora": time.strftime("%Y-%m-%dT%H:%M:%S-05:00"),
    }
    r = _session.post(SECUREGPT_AUTH_URL, data=payload, headers=headers, timeout=60, verify=REQUESTS_VERIFY_TLS)
    r.raise_for_status()
    return r.json()["access_token"]

def call_chat(messages: List[Dict[str, Any]], temperature: float = 0.2, max_tokens: int = 600) -> str:
    token = _get_auth_token()
    payload = {
        "api_version": SECUREGPT_API_VERSION,
        "model_url": SECUREGPT_CHAT_MODEL_URL or None,  # forwarded to proxy
        "messages": messages,
        "temperature": temperature,
        "max_tokens": max_tokens,
        "stream": False
    }
    payload = {k:v for k,v in payload.items() if v is not None}
    headers = _common_headers() | {"Authorization": f"Bearer {token}"}
    r = _session.post(SECUREGPT_PROXY_CHAT_URL, json=payload, headers=headers, timeout=120, verify=REQUESTS_VERIFY_TLS)
    r.raise_for_status()
    data = r.json()
    return data["choices"][0]["message"]["content"]

def call_embeddings(texts: List[str]) -> List[List[float]]:
    token = _get_auth_token()
    payload = {
        "api_version": SECUREGPT_API_VERSION,
        "model_url": SECUREGPT_EMBED_MODEL_URL or None,  # forwarded to proxy
        "input": texts,
    }
    payload = {k:v for k,v in payload.items() if v is not None}
    headers = _common_headers() | {"Authorization": f"Bearer {token}"}
    r = _session.post(SECUREGPT_PROXY_EMBED_URL, json=payload, headers=headers, timeout=120, verify=REQUESTS_VERIFY_TLS)
    r.raise_for_status()
    data = r.json()
    return [d["embedding"] for d in data["data"]]

# -------------------------
# Vector store helpers (document-level cosine)
# -------------------------
def load_meta() -> List[Dict[str, Any]]:
    if not META_PATH.exists(): return []
    lines = META_PATH.read_text(encoding="utf-8").splitlines()
    return [json.loads(l) for l in lines if l.strip()]

def load_matrix() -> Optional[np.ndarray]:
    return np.load(EMB_PATH) if EMB_PATH.exists() else None

def cosine_search(query_text: str, top_k: int = 8) -> List[Dict[str, Any]]:
    rows = load_meta()
    mat  = load_matrix()
    if not rows or mat is None or mat.size == 0:
        return []
    qv = np.array(call_embeddings([query_text])[0], dtype=np.float32)
    A  = mat.astype(np.float32)
    A /= (np.linalg.norm(A, axis=1, keepdims=True) + 1e-8)
    q  = qv / (np.linalg.norm(qv) + 1e-8)
    sims = A @ q
    order = np.argsort(-sims)[:top_k]
    return [{"name": rows[i]["name"], "path": rows[i]["path"], "score": float(sims[i])} for i in order]

def fuzzy_title_hits(proposed_title: str, cutoff: float = 0.84) -> List[Dict[str, Any]]:
    names = [os.path.basename(r["path"]) for r in load_meta()]
    hits = []
    for n in set(names):
        s = difflib.SequenceMatcher(a=(proposed_title or "").lower(), b=n.lower()).ratio()
        if s >= cutoff:
            hits.append({"name": n, "score": float(s)})
    return sorted(hits, key=lambda x: x["score"], reverse=True)

# -------------------------
# Models
# -------------------------
class AssessmentIn(BaseModel):
    user_request_text: str
    max_q: int = 10

class AssessmentOut(BaseModel):
    questions: List[Dict[str, Any]]

class VerifyIn(BaseModel):
    user_request_text: str
    answers: Dict[str, Any] = {}
    threshold: float = 0.82
    top_k: int = 8

# -------------------------
# Prompt utils
# -------------------------
def strip_code_fences(s: str) -> str:
    return re.sub(r"^```(?:json)?|```$", "", s.strip(), flags=re.MULTILINE).strip()

def build_candidate(user_text: str, answers: Dict[str, Any]) -> str:
    parts = [
        user_text,
        f"Tipo: {answers.get('doc_type','')}",
        f"Título: {answers.get('title','')}",
        f"Propósito: {answers.get('purpose','')}",
        f"Audiencia: {answers.get('audience','')}",
        f"Alcance: {answers.get('scope','')}",
        f"Dependencias: {answers.get('dependencies','')}",
        f"Normativa: {answers.get('regulatory','')}",
        f"Owner: {answers.get('owner_team','')}",
        f"Idiomas: {answers.get('languages','')}",
    ]
    return " | ".join([p for p in parts if p and p.strip()])

def gen_alt_titles(user_text: str, answers: Dict[str, Any], k: int = 6) -> List[str]:
    prompt = f"""
Given this creation intent:
- request="{user_text}"
- title="{answers.get('title','')}"
- doc_type="{answers.get('doc_type','')}"
- purpose="{answers.get('purpose','')}"
Generate up to {k} short alternative titles or keyword strings that a similar document might use.
Return JSON only: {{"candidates": ["...", "..."]}}
""".strip()
    try:
        raw = call_chat(
            [{"role":"system","content":"Return strict JSON only."},
             {"role":"user","content":prompt}],
            temperature=0.1, max_tokens=200
        )
        data = json.loads(strip_code_fences(raw))
        return [s for s in (data.get("candidates") or []) if isinstance(s, str)]
    except Exception:
        return []

# -------------------------
# Endpoints (Assessment + Verifier)
# -------------------------
@app.post("/create/assessment", response_model=AssessmentOut)
def create_assessment(body: AssessmentIn):
    system = "You ask concise, high-signal questions. Output strict JSON only."
    user = f"""
User wants to create a new document. Draft a compact questionnaire to collect
the minimum viable info to author v1 and route for approval.

Return JSON with this shape ONLY:
{{
  "questions": [
    {{"id":"q1","field":"doc_type","question":"…","hint":"…","required":true}},
    {{"id":"q2","field":"title","question":"…","required":true}},
    {{"id":"q3","field":"purpose","question":"…","required":true}},
    {{"id":"q4","field":"audience","question":"…","required":true}},
    {{"id":"q5","field":"scope","question":"…","required":true}},
    {{"id":"q6","field":"dependencies","question":"…","required":false}},
    {{"id":"q7","field":"regulatory","question":"…","required":false}},
    {{"id":"q8","field":"owner_team","question":"…","required":true}},
    {{"id":"q9","field":"deadline","question":"…","required":false}},
    {{"id":"q10","field":"languages","question":"…","hint":"ES, EN","required":false}}
  ]
}}
Rules:
- No prose outside JSON.
- <= {body.max_q} questions.
Context from user: "{body.user_request_text}"
""".strip()

    try:
        raw = call_chat(
            [{"role":"system","content":system},{"role":"user","content":user}],
            temperature=0.2, max_tokens=600
        )
        data = json.loads(strip_code_fences(raw))
        qs = data.get("questions") or []
    except Exception:
        qs = [
            {"id":"q1","field":"doc_type","question":"¿Qué tipo de documento necesitas (política, proceso, guía, plantilla)?","required":True},
            {"id":"q2","field":"title","question":"Título propuesto del documento","required":True},
            {"id":"q3","field":"purpose","question":"¿Cuál es el objetivo principal?","required":True},
            {"id":"q4","field":"audience","question":"¿Quiénes serán los lectores/usuarios?","required":True},
            {"id":"q5","field":"scope","question":"Alcance (qué incluye y qué no)","required":True},
            {"id":"q6","field":"dependencies","question":"Sistemas/Procesos relacionados o dependencias","required":False},
            {"id":"q7","field":"regulatory","question":"Normativa/controles aplicables (si aplica)","required":False},
            {"id":"q8","field":"owner_team","question":"Equipo/Área dueña del documento","required":True},
            {"id":"q9","field":"deadline","question":"¿Fecha objetivo?","required":False},
            {"id":"q10","field":"languages","question":"Idiomas requeridos (ES/EN)","required":False},
        ]

    out = []
    for i, q in enumerate(qs[: body.max_q], start=1):
        out.append({
            "id": str(q.get("id") or f"q{i}"),
            "field": str(q.get("field") or f"field_{i}"),
            "question": str(q.get("question") or "Provide value"),
            "hint": q.get("hint"),
            "required": bool(q.get("required", True)),
        })
    return {"questions": out}

@app.post("/create/verify")
def create_verify(body: VerifyIn):
    candidate = build_candidate(body.user_request_text, body.answers)
    alts = gen_alt_titles(body.user_request_text, body.answers, k=6)

    tried_qs = [candidate] + alts
    vector_results = []
    strong_any = False

    for q in tried_qs:
        hits = cosine_search(q, top_k=body.top_k)  # doc-level cosine
        strong = [h for h in hits if h["score"] >= body.threshold]
        if strong: strong_any = True
        vector_results.append({"query": q, "topk_docs": hits, "strong": strong})

    fuzzy = fuzzy_title_hits(body.answers.get("title", ""), cutoff=0.84)
    status = "duplicate_suspected" if strong_any or len(fuzzy) > 0 else "clear_to_create"

    return {
        "status": status,
        "tried_queries": tried_qs,
        "vector_results": vector_results,
        "fuzzy_title_hits": fuzzy
    }