# =========================
# Interactive Assessment & Verification (Create Mode)
# =========================
from dataclasses import dataclass, asdict
from typing import Optional
import difflib, json, re, numpy as np

# --- 0) Small IO adapter so you can plug this into CLI or your UI ---
class InteractionIO:
    """Default IO uses input()/print(). In your UI, override ask() to show prompts and collect user answers."""
    def ask(self, question: str, field: str, hint: Optional[str] = None, required: bool = True) -> str:
        msg = f"\n{question}\n"
        if hint: msg += f"(hint: {hint})\n"
        ans = input(msg + "> ").strip()
        while required and not ans:
            ans = input("(required) " + msg + "> ").strip()
        return ans

# --- 1) Generate clarifying questions with the chat model (JSON only) ---
def gen_clarifying_questions(user_request_text: str, max_q: int = 10) -> list[dict]:
    system = "You ask concise, high-signal questions. Output strict JSON only."
    user = f"""
User wants to create a new document. Draft a compact questionnaire to collect
the minimum viable info needed to author the first version and route for approval.

Return JSON with this shape ONLY:
{{
  "questions": [
    {{"id":"q1","field":"doc_type","question":"…","hint":"…","required":true}},
    {{"id":"q2","field":"title","question":"…","hint":"…","required":true}},
    {{"id":"q3","field":"purpose","question":"…","required":true}},
    {{"id":"q4","field":"audience","question":"…","required":true}},
    {{"id":"q5","field":"scope","question":"…","required":true}},
    {{"id":"q6","field":"dependencies","question":"…","required":false}},
    {{"id":"q7","field":"regulatory","question":"…","required":false}},
    {{"id":"q8","field":"owner_team","question":"…","required":true}},
    {{"id":"q9","field":"deadline","question":"…","required":false}},
    {{"id":"q10","field":"languages","question":"…","hint":"e.g., ES, EN","required":false}}
  ]
}}
Rules:
- No prose outside JSON.
- <= {max_q} questions.
- Prefer closed or short-answer prompts.
Context from user: "{user_request_text}"
    """.strip()

    raw = call_chat(
        [{"role":"system","content":system},
         {"role":"user","content":user}],
        temperature=0.2, max_tokens=600
    )
    # Be tolerant to accidental markdown fences
    raw = raw.strip()
    raw = re.sub(r"^```(?:json)?|```$", "", raw, flags=re.MULTILINE).strip()
    try:
        data = json.loads(raw)
        qs = data.get("questions") or []
    except Exception:
        # Safe fallback
        qs = [
            {"id":"q1","field":"doc_type","question":"¿Qué tipo de documento necesitas (política, proceso, guía, plantilla)?","required":True},
            {"id":"q2","field":"title","question":"Título propuesto del documento","required":True},
            {"id":"q3","field":"purpose","question":"¿Cuál es el objetivo principal?","required":True},
            {"id":"q4","field":"audience","question":"¿Quiénes serán los lectores/usuarios?","required":True},
            {"id":"q5","field":"scope","question":"Alcance (qué incluye y qué no)","required":True},
            {"id":"q6","field":"dependencies","question":"Sistemas/Procesos relacionados o dependencias","required":False},
            {"id":"q7","field":"regulatory","question":"Normativa/controles aplicables (si aplica)","required":False},
            {"id":"q8","field":"owner_team","question":"Equipo/Área dueña del documento","required":True},
            {"id":"q9","field":"deadline","question":"¿Fecha objetivo?","required":False},
            {"id":"q10","field":"languages","question":"Idiomas requeridos (ES/EN)","required":False},
        ]
    # Ensure structure and bounds
    cleaned = []
    for q in qs[:max_q]:
        cleaned.append({
            "id":        str(q.get("id") or f"q{len(cleaned)+1}"),
            "field":     str(q.get("field") or f"field_{len(cleaned)+1}"),
            "question":  str(q.get("question") or "Provide value"),
            "hint":      q.get("hint"),
            "required":  bool(q.get("required", True))
        })
    return cleaned

# --- 2) Ask user and collect answers ---
def run_assessment(io: InteractionIO, questions: list[dict]) -> dict:
    answers = {}
    for q in questions:
        ans = io.ask(q["question"], q["field"], q.get("hint"), q.get("required", True))
        answers[q["field"]] = ans
    return answers

# --- 3) Build a richer candidate description from answers (to re-check existence) ---
def build_candidate_query(user_request_text: str, answers: dict) -> str:
    parts = [
        user_request_text,
        f"Tipo: {answers.get('doc_type','')}",
        f"Título: {answers.get('title','')}",
        f"Propósito: {answers.get('purpose','')}",
        f"Audiencia: {answers.get('audience','')}",
        f"Alcance: {answers.get('scope','')}",
        f"Dependencias: {answers.get('dependencies','')}",
        f"Normativa: {answers.get('regulatory','')}",
        f"Owner: {answers.get('owner_team','')}",
        f"Idiomas: {answers.get('languages','')}",
    ]
    return " | ".join([p for p in parts if p and p.strip()])

# --- 4) Ask the model for synonyms/alternative titles to broaden the search ---
def gen_alt_titles_and_keywords(user_request_text: str, answers: dict, k: int = 6) -> list[str]:
    prompt = f"""
Given this creation intent:
- request="{user_request_text}"
- title="{answers.get('title','')}"
- doc_type="{answers.get('doc_type','')}"
- purpose="{answers.get('purpose','')}"
Generate up to {k} short alternative titles or keyword strings that a similar document might have
(e.g., synonyms, legacy names, abbreviations). JSON only:
{{"candidates": ["...", "..."]}}
""".strip()
    raw = call_chat(
        [{"role":"system","content":"Return strict JSON only."},
         {"role":"user","content":prompt}],
        temperature=0.1, max_tokens=200
    ).strip()
    raw = re.sub(r"^```(?:json)?|```$", "", raw, flags=re.MULTILINE).strip()
    try:
        data = json.loads(raw)
        return [s for s in (data.get("candidates") or []) if isinstance(s, str)]
    except Exception:
        return []

# --- 5) Shallow fuzzy check on file names (title vs existing doc names) ---
def fuzzy_title_scan(proposed_title: str, existing_names: list[str], cutoff: float = 0.82) -> list[tuple[str,float]]:
    if not proposed_title: return []
    matches = []
    for name in existing_names:
        score = difflib.SequenceMatcher(a=proposed_title.lower(), b=name.lower()).ratio()
        if score >= cutoff:
            matches.append((name, score))
    return sorted(matches, key=lambda x: x[1], reverse=True)

# --- 6) Template recommendation + file naming (via chat) ---
@dataclass
class TemplateDecision:
    proceed: bool
    reason: str
    recommended_template: Optional[str]
    filename_suggestion: Optional[str]
    sections: list[str]
    metadata: dict

def recommend_template_and_filename(answers: dict, evidence: dict) -> TemplateDecision:
    """Asks the chat model to suggest the template and a safe file name."""
    context = {
        "answers": answers,
        "evidence": evidence  # can include found matches from re-checks
    }
    system = "You output strict JSON only. Be concrete and brief."
    user = f"""
Decide the best template and a safe filename for a new document based on:
{json.dumps(context, ensure_ascii=False)}

Return JSON only:
{{
  "proceed": true|false,
  "reason": "short line",
  "recommended_template": "Policy|Process|Guideline|Template|SOP|Other",
  "filename_suggestion": "kebab-or-underscore-friendly-name-v1.0.docx",
  "sections": ["Intro","Scope","Roles","Controls","Exceptions","References"],
  "metadata": {{
      "owner_team": "...",
      "audience": "...",
      "languages": ["ES","EN"],
      "regulatory": "...",
      "risk": "Low|Medium|High"
  }}
}}
Rules:
- If you think a near-duplicate exists, set proceed=false and explain briefly in reason.
- Filename must be unique-friendly (no spaces, concise).
""".strip()

    raw = call_chat(
        [{"role":"system","content":system},
         {"role":"user","content":user}],
        temperature=0.2, max_tokens=500
    ).strip()
    raw = re.sub(r"^```(?:json)?|```$", "", raw, flags=re.MULTILINE).strip()
    try:
        data = json.loads(raw)
    except Exception:
        data = {
            "proceed": True,
            "reason": "default",
            "recommended_template": answers.get("doc_type","Template"),
            "filename_suggestion": "new-document-v1.0.docx",
            "sections": ["Intro","Scope","Roles","Controls","Exceptions","References"],
            "metadata": {
                "owner_team": answers.get("owner_team",""),
                "audience": answers.get("audience",""),
                "languages": (answers.get("languages","") or "ES").split(","),
                "regulatory": answers.get("regulatory",""),
                "risk": "Medium"
            }
        }
    return TemplateDecision(
        proceed=bool(data.get("proceed", True)),
        reason=str(data.get("reason","")),
        recommended_template=data.get("recommended_template"),
        filename_suggestion=data.get("filename_suggestion"),
        sections=list(data.get("sections") or []),
        metadata=dict(data.get("metadata") or {})
    )

# --- 7) Orchestrator: full interactive session when precheck found nothing ---
def run_create_mode_session(
    user_request_text: str,
    io: InteractionIO | None = None,
    strong_threshold: float = 0.82
) -> dict:
    """
    Main entry for Create mode when precheck returned no strong matches.
    Steps:
      1) Ask clarifying questions (via SecureGPT) and collect answers
      2) Rebuild a candidate description & broaden search with alt titles/keywords
      3) Re-run existence checks (vector search + fuzzy filename)
      4) Recommend template & filename; return a structured payload
    """
    io = io or InteractionIO()

    # 1) Questions -> Answers
    questions = gen_clarifying_questions(user_request_text)
    answers   = run_assessment(io, questions)

    # 2) Build richer query & alternatives
    candidate = build_candidate_query(user_request_text, answers)
    alts = gen_alt_titles_and_keywords(user_request_text, answers, k=6)

    # 3) Re-check existence (vector search with broadened queries)
    all_hits = []
    tried_queries = [candidate] + alts
    for q in tried_queries:
        res = create_mode_precheck(q, threshold=strong_threshold, top_k=5)
        all_hits.append({"query": q, "strong": res["strong_matches"], "topk": res["topk"]})

    # Also fuzzy-scan titles vs current filenames (only names from first pass is ok)
    meta_rows = load_meta()
    existing_names = [r["name"] for r in meta_rows]
    fuzzy_hits = fuzzy_title_scan(answers.get("title",""), existing_names, cutoff=0.84)

    evidence = {
        "tried_queries": tried_queries,
        "vector_hits": all_hits,
        "fuzzy_title_hits": [{"name": n, "score": s} for (n,s) in fuzzy_hits]
    }

    # If any strong vector match surfaced now, flag it
    any_strong = any((block["strong"] for block in all_hits if block["strong"]))

    # 4) Template recommendation
    decision = recommend_template_and_filename(answers, evidence)

    payload = {
        "request": user_request_text,
        "answers": answers,
        "evidence": evidence,
        "decision": asdict(decision),
        "status": "duplicate_suspected" if any_strong or not decision.proceed else "proceed_new_doc"
    }
    return payload

# --- 8) Quick example (CLI) ---
if __name__ == "__main__":
    # Simulate the branch when precheck returned nothing:
    user_intent = "Quiero crear una nueva Política de Uso de IA para proveedores externos"
    result = run_create_mode_session(user_intent)
    print("\n=== SESSION RESULT ===")
    print(json.dumps(result, ensure_ascii=False, indent=2))